{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5b9bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "model_dir = \"model/\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=model_dir\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84eaae8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3798364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are JaLmg, a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "å°†ä¸€ä¸ªå…³äºå¤§å­¦ç”Ÿçš„ç¬‘è¯<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"å°†ä¸€ä¸ªå…³äºå¤§å­¦ç”Ÿçš„ç¬‘è¯\"\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are JaLmg, a helpful assistant.\"},\n",
    "    {\"role\":\"user\",\"content\":prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c4b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,  22871,     43,  12311,     11,\n",
      "            264,  10950,  17847,     13, 151645,    198, 151644,    872,    198,\n",
      "          44063,  46944, 101888, 104422,   9370, 109959, 151645,    198, 151644,\n",
      "          77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9ac584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘æä¾›ä¸€ä¸ªå…³äºå¤§å­¦ç”Ÿçš„ç¬‘è¯ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®ä¿ç¬‘è¯é€‚åˆæ‰€æœ‰å¹´é¾„æ®µï¼Œé¿å…ä»»ä½•å¯èƒ½å¼•èµ·ä¸é€‚çš„å†…å®¹ã€‚å¤§å­¦ç”Ÿè¿™ä¸ªç¾¤ä½“å¯èƒ½æœ‰å„ç§æœ‰è¶£çš„èƒŒæ™¯ï¼Œä½†è¦æ³¨æ„å¹½é»˜çš„è¡¨è¾¾æ–¹å¼ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘å¾—è€ƒè™‘ç¬‘è¯çš„ç»“æ„ã€‚é€šå¸¸ï¼Œç¬‘è¯ä¼šæœ‰ä¸€ä¸ªé—®é¢˜æˆ–æƒ…æ™¯ï¼Œç„¶åé€šè¿‡åè½¬æˆ–æ„æƒ³ä¸åˆ°çš„æ–¹å¼è§£å†³ã€‚æ¯”å¦‚ï¼Œå¤§å­¦ç”Ÿå¯èƒ½é‡åˆ°ä¸€äº›å¸¸è§çš„é—®é¢˜ï¼Œä½†ç”¨å¹½é»˜çš„æ–¹å¼è¡¨è¾¾ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘éœ€è¦æ£€æŸ¥ç¬‘è¯çš„è¿è´¯æ€§å’Œé€»è¾‘æ€§ã€‚ç¡®ä¿ç¬‘è¯ä¸ä¼šè®©äººæ„Ÿåˆ°å›°æƒ‘æˆ–ä¸é€‚ã€‚åŒæ—¶ï¼Œè¦ä¿æŒè¯­è¨€ç®€æ´ï¼Œè®©è¯»è€…å®¹æ˜“ç†è§£ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œè¿˜è¦æ³¨æ„ç¬‘è¯çš„é•¿åº¦ï¼Œä¸è¦å¤ªé•¿ï¼Œé€‚åˆå¿«é€Ÿé˜…è¯»ã€‚å¯èƒ½éœ€è¦è°ƒæ•´å¥å­çš„ç»“æ„ï¼Œä½¿å…¶æ›´æµç•…ã€‚\n",
      "\n",
      "æœ€åï¼Œç¡®ä¿ç¬‘è¯ç¬¦åˆå¤§å­¦ç”Ÿçš„å¸¸è§åœºæ™¯ï¼Œæ¯”å¦‚æ ¡å›­ç”Ÿæ´»ã€å­¦ä¹ å‹åŠ›ç­‰ï¼Œè¿™æ ·æ›´å®¹æ˜“å¼•èµ·å…±é¸£ã€‚åŒæ—¶ï¼ŒåŠ å…¥ä¸€äº›ç”ŸåŠ¨çš„ç»†èŠ‚ï¼Œæ¯”å¦‚è¡¨æƒ…ã€åŠ¨ä½œï¼Œå¯ä»¥è®©ç¬‘è¯æ›´ç”ŸåŠ¨æœ‰è¶£ã€‚\n",
      "\n",
      "æ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘éœ€è¦æ„æ€ä¸€ä¸ªç®€å•æ˜äº†ã€æœ‰åè½¬å’Œå¹½é»˜æ„Ÿçš„ç¬‘è¯ï¼Œå›´ç»•å¤§å­¦ç”Ÿçš„ç”Ÿæ´»æˆ–é—®é¢˜å±•å¼€ï¼Œç¡®ä¿å†…å®¹å¥åº·ç§¯æï¼Œé€‚åˆç›®æ ‡è¯»è€…ã€‚\n",
      "</think>\n",
      "\n",
      "å½“ç„¶ï¼è¿™æ˜¯ä¸€ä¸ªå…³äºå¤§å­¦ç”Ÿçš„å¹½é»˜ç¬‘è¯ï¼š\n",
      "\n",
      "**\"ä¸ºä»€ä¹ˆå¤§å­¦ç”Ÿè¦è€ƒç ”ç©¶ç”Ÿï¼Ÿå› ä¸ºâ€¦â€¦å› ä¸ºâ€¦â€¦å› ä¸ºâ€¦â€¦\"**\n",
      "\n",
      "ï¼ˆçªç„¶ï¼Œä¸€ä¸ªç”·ç”Ÿä»æ•™å®¤é‡Œè·‘å‡ºæ¥ï¼Œæ‰‹é‡Œæ‹¿ç€ä¸€ä¸ªå‘å…‰çš„ç›’å­ï¼Œå˜´é‡Œå–Šç€ï¼š\"ä¸ºä»€ä¹ˆå¤§å­¦ç”Ÿè¦è€ƒç ”ç©¶ç”Ÿï¼Ÿå› ä¸ºâ€¦â€¦å› ä¸ºâ€¦â€¦å› ä¸ºâ€¦â€¦\"ï¼‰\n",
      "\n",
      "ï¼ˆé•œå¤´æ‹‰è¿œï¼Œä¸€ä¸ªç©¿ç€æ ¡æœçš„ç”·ç”Ÿç«™åœ¨æ•™å®¤é—¨å£ï¼Œæ‰‹é‡Œæ‹¿ç€ä¸€ä¸ªå‘å…‰çš„ç›’å­ï¼Œç¬‘å¾—å¤§ç¬‘ï¼‰  \n",
      "\n",
      "**\"å› ä¸ºâ€¦â€¦å› ä¸ºâ€¦â€¦å› ä¸ºâ€¦â€¦\"**  \n",
      "\n",
      "ï¼ˆé•œå¤´å®šæ ¼ï¼Œç”·ç”Ÿéœ²å‡ºå¼€å¿ƒçš„ç¬‘å®¹ï¼Œç›’å­é‡Œæ‰å‡ºä¸€å¼ å†™ç€\"**è€ƒç ”ç©¶ç”Ÿ**\"çš„çº¸æ¡ã€‚ï¼‰  \n",
      "\n",
      "**ç¬‘ç‚¹è§£æ**ï¼š  \n",
      "- é€šè¿‡åè½¬ï¼ˆç›’å­é‡Œçš„çº¸æ¡ï¼‰åˆ¶é€ æ‚¬å¿µï¼Œ  \n",
      "- ç”¨\"ä¸ºä»€ä¹ˆ\"å¼•å‡ºè’è¯çš„é€»è¾‘ï¼Œ  \n",
      "- ç»“å°¾çš„\"è€ƒç ”ç©¶ç”Ÿ\"ç‚¹æ˜ä¸»é¢˜ï¼Œ  \n",
      "- ç”¨è¡¨æƒ…å’ŒåŠ¨ä½œå¢å¼ºç”»é¢æ„Ÿã€‚\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs[\"input_ids\"], generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c10b09d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®å®šç”¨æˆ·çš„éœ€æ±‚æ˜¯ä»€ä¹ˆã€‚ä»–ä»¬å¯èƒ½æ˜¯åœ¨æµ‹è¯•æˆ‘çš„èƒ½åŠ›ï¼Œæˆ–è€…æƒ³äº†è§£æˆ‘çš„èƒŒæ™¯ï¼Œæˆ–è€…åªæ˜¯å¥½å¥‡æˆ‘çš„èº«ä»½ã€‚ä½œä¸ºAIåŠ©æ‰‹ï¼Œæˆ‘åº”è¯¥ä¿æŒè‡ªç„¶å’Œå‹å¥½çš„è¯­æ°”ï¼ŒåŒæ—¶æä¾›æœ‰ç”¨çš„ä¿¡æ¯ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„èƒŒæ™¯ã€‚ä»–ä»¬å¯èƒ½æ˜¯å­¦ç”Ÿã€å·¥ä½œäººå£«ï¼Œæˆ–è€…å¯¹AIæ„Ÿå…´è¶£çš„äººã€‚ä¸åŒçš„ç”¨æˆ·å¯èƒ½éœ€è¦ä¸åŒçš„å›ç­”ã€‚æ¯”å¦‚ï¼Œå­¦ç”Ÿå¯èƒ½éœ€è¦æ›´ä¸“ä¸šçš„ä»‹ç»ï¼Œè€ŒèŒåœºäººå£«å¯èƒ½æ›´å…³æ³¨åŠŸèƒ½å’Œä¼˜åŠ¿ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘éœ€è¦ç¡®ä¿å›ç­”çš„ä¿¡æ¯å‡†ç¡®ä¸”å…¨é¢ã€‚è¦æ¶µç›–æˆ‘çš„ä¸»è¦åŠŸèƒ½ï¼Œæ¯”å¦‚çŸ¥è¯†åº“ã€å¯¹è¯å†å²ã€å¤šè¯­è¨€æ”¯æŒç­‰ã€‚åŒæ—¶ï¼Œè¦å¼ºè°ƒæˆ‘çš„ä¼˜ç‚¹ï¼Œæ¯”å¦‚å¿«é€Ÿå“åº”ã€çµæ´»è°ƒæ•´ã€éšç§ä¿æŠ¤ç­‰ï¼Œä»¥å±•ç¤ºæˆ‘çš„å¯é æ€§ã€‚\n",
      "\n",
      "è¿˜è¦æ³¨æ„ç”¨æˆ·å¯èƒ½çš„æ·±å±‚éœ€æ±‚ã€‚ä»–ä»¬å¯èƒ½å¸Œæœ›äº†è§£å¦‚ä½•ä½¿ç”¨æˆ‘çš„æœåŠ¡ï¼Œæˆ–è€…å¦‚ä½•ä¸æˆ‘äº’åŠ¨ã€‚å› æ­¤ï¼Œåœ¨å›ç­”ä¸­å¯ä»¥æä¾›ä¸€äº›ä½¿ç”¨å»ºè®®ï¼Œæ¯”å¦‚å¦‚ä½•å¼€å§‹å¯¹è¯ï¼Œæˆ–è€…å¦‚ä½•ä¼˜åŒ–æˆ‘çš„å›ç­”ã€‚\n",
      "\n",
      "æœ€åï¼Œä¿æŒå›ç­”ç®€æ´æ˜äº†ï¼Œé¿å…ä½¿ç”¨è¿‡äºæŠ€æœ¯åŒ–çš„æœ¯è¯­ï¼Œè®©æ‰€æœ‰ç”¨æˆ·éƒ½èƒ½è½»æ¾ç†è§£ã€‚åŒæ—¶ï¼Œä¿æŒç¤¼è²Œå’Œä¸“ä¸šçš„å½¢è±¡ï¼Œç¡®ä¿ä¿¡æ¯ä¼ è¾¾æ¸…æ™°ã€‚\n",
      "</think>\n",
      "\n",
      "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥å¸®ä½ è§£ç­”å„ç§é—®é¢˜ï¼Œä»å­¦ä¹ åˆ°å·¥ä½œï¼Œä»æ—¥å¸¸äº‹åŠ¡åˆ°åˆ›æ„çµæ„Ÿã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"user\",\"content\":\"è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\"}\n",
    "]\n",
    "\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612dc48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
